#!/usr/bin/env python3
"""
ENHANCED STATISTICAL ANALYSIS ENGINE - PURE DATA-DRIVEN APPROACH
==============================================================
âœ… Z-score normalization for better comparisons
âœ… Confidence intervals for adjustments
âœ… Multi-factor regression weights
âœ… No artificial boosting - only statistical significance
"""

import os
import re
from dataclasses import dataclass
from difflib import SequenceMatcher
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np
import pandas as pd

# Try to import scipy for better statistical functions
try:
    pass

    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("âš ï¸ scipy not available - using basic statistics")


@dataclass
class ConfidenceConfig:
    """Data source confidence configuration based on historical accuracy"""

    statcast: float = 0.90  # Official MLB metrics - highest confidence
    vegas: float = 0.85  # Market-tested odds - high confidence
    dff: float = 0.75  # Expert opinion - good confidence
    park_factors: float = 0.80  # Historical venue data - high confidence


class EnhancedStatcastScoring:
    """Enhanced Statcast composite scoring with z-score normalization"""

    def __init__(self):
        # League baseline values with standard deviations
        self.baselines = {
            "hitter": {
                "xwOBA": {"mean": 0.320, "std": 0.040},
                "hard_hit_pct": {"mean": 37.0, "std": 8.0},
                "barrel_pct": {"mean": 8.5, "std": 4.0},
                "avg_exit_velocity": {"mean": 88.5, "std": 2.5},
                "walk_rate": {"mean": 8.5, "std": 3.0},
                "k_rate": {"mean": 23.0, "std": 5.0},
            },
            "pitcher": {
                "xwOBA_against": {"mean": 0.315, "std": 0.035},
                "k_rate": {"mean": 22.0, "std": 5.0},
                "whiff_rate": {"mean": 25.0, "std": 5.0},
                "hard_hit_against": {"mean": 35.0, "std": 7.0},
            },
        }

    def calculate_z_score(self, value: float, metric: str, player_type: str) -> float:
        """Calculate z-score for a metric"""
        baseline = self.baselines[player_type].get(metric)
        if not baseline:
            return 0.0

        mean = baseline["mean"]
        std = baseline["std"]

        if std == 0:
            return 0.0

        return (value - mean) / std

    def calculate_hitter_composite_score(self, statcast_data: Dict, position: str) -> float:
        """Calculate composite score using z-scores"""
        if not statcast_data:
            return 0.0

        try:
            # Calculate z-scores for each metric
            z_scores = {}

            # xwOBA z-score
            xwoba = statcast_data.get("xwOBA", self.baselines["hitter"]["xwOBA"]["mean"])
            z_scores["xwOBA"] = self.calculate_z_score(xwoba, "xwOBA", "hitter")

            # Hard hit z-score
            hard_hit = statcast_data.get(
                "Hard_Hit", self.baselines["hitter"]["hard_hit_pct"]["mean"]
            )
            z_scores["hard_hit"] = self.calculate_z_score(hard_hit, "hard_hit_pct", "hitter")

            # Barrel z-score
            barrel = statcast_data.get("Barrel", self.baselines["hitter"]["barrel_pct"]["mean"])
            z_scores["barrel"] = self.calculate_z_score(barrel, "barrel_pct", "hitter")

            # Exit velocity z-score
            exit_velo = statcast_data.get(
                "avg_exit_velocity", self.baselines["hitter"]["avg_exit_velocity"]["mean"]
            )
            z_scores["exit_velo"] = self.calculate_z_score(exit_velo, "avg_exit_velocity", "hitter")

            # Discipline z-score (BB-K differential)
            walk_rate = statcast_data.get("BB", self.baselines["hitter"]["walk_rate"]["mean"])
            k_rate = statcast_data.get("K", self.baselines["hitter"]["k_rate"]["mean"])
            bb_k_diff = walk_rate - k_rate
            expected_diff = (
                self.baselines["hitter"]["walk_rate"]["mean"]
                - self.baselines["hitter"]["k_rate"]["mean"]
            )
            z_scores["discipline"] = (bb_k_diff - expected_diff) / 5.0  # Approximate std

            # Position-specific weighting based on statistical importance
            if position in ["1B", "OF"]:  # Power positions
                weights = {
                    "xwOBA": 0.25,
                    "hard_hit": 0.25,
                    "barrel": 0.30,
                    "exit_velo": 0.15,
                    "discipline": 0.05,
                }
            elif position in ["2B", "SS"]:  # Contact positions
                weights = {
                    "xwOBA": 0.35,
                    "hard_hit": 0.15,
                    "barrel": 0.15,
                    "exit_velo": 0.10,
                    "discipline": 0.25,
                }
            elif position == "C":  # Catchers
                weights = {
                    "xwOBA": 0.30,
                    "hard_hit": 0.20,
                    "barrel": 0.20,
                    "exit_velo": 0.15,
                    "discipline": 0.15,
                }
            else:  # 3B and others
                weights = {
                    "xwOBA": 0.30,
                    "hard_hit": 0.20,
                    "barrel": 0.25,
                    "exit_velo": 0.15,
                    "discipline": 0.10,
                }

            # Calculate weighted composite z-score
            composite_z = sum(z_scores[metric] * weights.get(metric, 0) for metric in z_scores)

            # Convert to adjustment score (each SD = ~5 points)
            composite_score = composite_z * 5.0

            return np.clip(composite_score, -15.0, 15.0)

        except Exception as e:
            print(f"âš ï¸ Error calculating hitter composite score: {e}")
            return 0.0

    def calculate_pitcher_composite_score(self, statcast_data: Dict) -> float:
        """Calculate pitcher composite score using z-scores"""
        if not statcast_data:
            return 0.0

        try:
            # Calculate z-scores
            z_scores = {}

            # xwOBA against (lower is better for pitchers)
            xwoba_against = statcast_data.get(
                "xwOBA", self.baselines["pitcher"]["xwOBA_against"]["mean"]
            )
            z_scores["xwOBA"] = -self.calculate_z_score(xwoba_against, "xwOBA_against", "pitcher")

            # K rate z-score
            k_rate = statcast_data.get("K", self.baselines["pitcher"]["k_rate"]["mean"])
            z_scores["k_rate"] = self.calculate_z_score(k_rate, "k_rate", "pitcher")

            # Whiff rate z-score
            whiff_rate = statcast_data.get("Whiff", self.baselines["pitcher"]["whiff_rate"]["mean"])
            z_scores["whiff"] = self.calculate_z_score(whiff_rate, "whiff_rate", "pitcher")

            # Hard hit against (lower is better)
            hard_hit = statcast_data.get(
                "Hard_Hit", self.baselines["pitcher"]["hard_hit_against"]["mean"]
            )
            z_scores["hard_hit"] = -self.calculate_z_score(hard_hit, "hard_hit_against", "pitcher")

            # Pitcher weights based on statistical importance
            weights = {
                "xwOBA": 0.40,  # Most predictive
                "k_rate": 0.30,  # Very important
                "whiff": 0.20,  # Good indicator
                "hard_hit": 0.10,  # Supporting metric
            }

            # Calculate weighted composite
            composite_z = sum(z_scores[metric] * weights.get(metric, 0) for metric in z_scores)

            # Convert to adjustment score
            composite_score = composite_z * 4.0  # Slightly less variance for pitchers

            return np.clip(composite_score, -12.0, 12.0)

        except Exception as e:
            print(f"âš ï¸ Error calculating pitcher composite score: {e}")
            return 0.0


class VariableConfidenceProcessor:
    """Pure statistical adjustment processor"""

    def __init__(self, config: ConfidenceConfig = None):
        self.config = config or ConfidenceConfig()
        self.statcast_scorer = EnhancedStatcastScoring()

        # Statistical significance thresholds
        self.significance_thresholds = {
            "statcast": 1.5,  # 1.5 standard deviations
            "vegas": 0.20,  # 20% deviation from mean
            "dff": 0.15,  # 15% difference required
        }

    def calculate_enhanced_adjustment(
        self, player, max_total_adjustment: float = 0.20
    ) -> Tuple[float, Dict]:
        """Calculate statistically significant adjustments only"""

        adjustments = {"statcast": 0.0, "vegas": 0.0, "dff": 0.0, "total": 0.0, "breakdown": {}}

        try:
            # 1. STATCAST - Z-score based adjustments
            if hasattr(player, "statcast_data") and player.statcast_data:
                if player.primary_position == "P":
                    statcast_score = self.statcast_scorer.calculate_pitcher_composite_score(
                        player.statcast_data
                    )
                else:
                    statcast_score = self.statcast_scorer.calculate_hitter_composite_score(
                        player.statcast_data, player.primary_position
                    )

                # Only adjust if statistically significant (z > 1.5)
                if (
                    abs(statcast_score) > self.significance_thresholds["statcast"] * 5
                ):  # Convert back from points
                    # Scale adjustment by confidence
                    statcast_adjustment = (statcast_score / 100) * self.config.statcast
                    adjustments["statcast"] = np.clip(statcast_adjustment, -0.12, 0.12)

                    adjustments["breakdown"]["statcast"] = {
                        "composite_score": statcast_score,
                        "z_score": statcast_score / 5.0,  # Approximate z-score
                        "confidence": self.config.statcast,
                        "adjustment": adjustments["statcast"],
                    }

            # 2. VEGAS - Statistical deviation from mean totals
            if hasattr(player, "vegas_data") and player.vegas_data:
                team_total = player.vegas_data.get("team_total", 4.5)
                opp_total = player.vegas_data.get("opponent_total", 4.5)

                # Calculate deviation from mean (4.5 runs)
                mean_total = 4.5

                if player.primary_position == "P":
                    # For pitchers, use opponent total
                    deviation = (opp_total - mean_total) / mean_total

                    if abs(deviation) > self.significance_thresholds["vegas"]:
                        # Negative correlation for pitchers (high runs = bad)
                        vegas_adjustment = -deviation * 0.5 * self.config.vegas
                        adjustments["vegas"] = np.clip(vegas_adjustment, -0.10, 0.10)
                else:
                    # For hitters, use team total
                    deviation = (team_total - mean_total) / mean_total

                    if abs(deviation) > self.significance_thresholds["vegas"]:
                        # Positive correlation for hitters (high runs = good)
                        vegas_adjustment = deviation * 0.5 * self.config.vegas
                        adjustments["vegas"] = np.clip(vegas_adjustment, -0.10, 0.10)

                if abs(adjustments["vegas"]) > 0.01:
                    adjustments["breakdown"]["vegas"] = {
                        "team_total": team_total,
                        "opp_total": opp_total,
                        "deviation": deviation * 100,  # As percentage
                        "confidence": self.config.vegas,
                        "adjustment": adjustments["vegas"],
                    }

            # 3. DFF - Statistical difference from base projection
            if hasattr(player, "dff_data") and player.dff_data:
                dff_projection = player.dff_data.get("ppg_projection", 0)
                base_score = getattr(player, "base_score", player.enhanced_score)

                if dff_projection > 0 and base_score > 0:
                    # Calculate percentage difference
                    pct_diff = (dff_projection - base_score) / base_score

                    # Only adjust if statistically significant
                    if abs(pct_diff) > self.significance_thresholds["dff"]:
                        # Use confidence scaling
                        dff_adjustment = pct_diff * 0.5 * self.config.dff
                        adjustments["dff"] = np.clip(dff_adjustment, -0.10, 0.10)

                        adjustments["breakdown"]["dff"] = {
                            "dff_projection": dff_projection,
                            "base_projection": base_score,
                            "pct_difference": pct_diff * 100,
                            "confidence": self.config.dff,
                            "adjustment": adjustments["dff"],
                        }

                        # Add ceiling/floor if available
                        if "ceiling" in player.dff_data:
                            adjustments["breakdown"]["dff"]["ceiling"] = player.dff_data["ceiling"]
                        if "floor" in player.dff_data:
                            adjustments["breakdown"]["dff"]["floor"] = player.dff_data["floor"]

            # Calculate total adjustment
            total_adjustment = adjustments["statcast"] + adjustments["vegas"] + adjustments["dff"]

            # Only apply if total adjustment is meaningful
            if abs(total_adjustment) > 0.01:
                # Apply smart capping
                if abs(total_adjustment) > max_total_adjustment:
                    scaling_factor = max_total_adjustment / abs(total_adjustment)
                    total_adjustment *= scaling_factor
                    for key in ["statcast", "vegas", "dff"]:
                        adjustments[key] *= scaling_factor

                adjustments["total"] = total_adjustment

            return adjustments["total"], adjustments

        except Exception as e:
            print(
                f"âš ï¸ Error in adjustment calculation for {getattr(player, 'name', 'Unknown')}: {e}"
            )
            return 0.0, adjustments


class DFFRankingsProcessor:
    """Handles DFF rankings integration with advanced name matching"""

    def __init__(self):
        self.name_similarity_threshold = 0.85

    def apply_dff_rankings(self, players: List[Any], dff_file_path: str) -> bool:
        """Apply DFF rankings with name matching verification"""
        if not dff_file_path or not os.path.exists(dff_file_path):
            print("âš ï¸ No DFF file provided or file not found")
            return False

        try:
            print(f"ðŸŽ¯ Loading DFF rankings: {Path(dff_file_path).name}")
            df = pd.read_csv(dff_file_path)

            # First, show what columns we found
            print(f"   DFF columns: {list(df.columns)}")

            matches = 0
            significant_adjustments = 0
            no_matches = []

            for _, row in df.iterrows():
                try:
                    # Get player name - try multiple patterns
                    player_name = None
                    for col in df.columns:
                        col_lower = col.lower()
                        if any(term in col_lower for term in ["player", "name", "playername"]):
                            candidate = str(row[col]).strip()
                            if candidate and candidate.lower() != "nan":
                                player_name = candidate
                                break

                    if not player_name:
                        continue

                    # Clean DFF name (remove team abbreviations, extra spaces)
                    dff_name = re.sub(r"\s*\([^)]*\)", "", player_name).strip()
                    dff_name = " ".join(dff_name.split())

                    # Find best match
                    best_match = None
                    best_score = 0

                    for player in players:
                        # Try exact match first
                        if player.name.lower() == dff_name.lower():
                            best_match = player
                            best_score = 1.0
                            break

                        # Then try similarity
                        similarity = self._name_similarity(dff_name, player.name)
                        if similarity > best_score and similarity >= self.name_similarity_threshold:
                            best_score = similarity
                            best_match = player

                    if best_match:
                        dff_data = {}

                        # Extract all relevant columns
                        for col in df.columns:
                            col_lower = col.lower()
                            try:
                                if any(
                                    term in col_lower
                                    for term in ["projection", "ppg", "points", "pts"]
                                ):
                                    value = float(row[col])
                                    if value > 0:
                                        dff_data["ppg_projection"] = value
                                elif "ownership" in col_lower:
                                    ownership_str = str(row[col]).replace("%", "").strip()
                                    if ownership_str:
                                        dff_data["ownership"] = float(ownership_str)
                                elif "ceiling" in col_lower:
                                    dff_data["ceiling"] = float(row[col])
                                elif "floor" in col_lower:
                                    dff_data["floor"] = float(row[col])
                            except:
                                pass

                        if dff_data and "ppg_projection" in dff_data:
                            dff_proj = dff_data["ppg_projection"]
                            base_proj = getattr(best_match, "base_score", best_match.enhanced_score)

                            if base_proj > 0:
                                pct_diff = (dff_proj - base_proj) / base_proj

                                # Apply even if small difference - let the stats engine decide
                                if hasattr(best_match, "apply_dff_data"):
                                    best_match.apply_dff_data(dff_data)
                                else:
                                    best_match.dff_data = dff_data

                                matches += 1

                                # Show match details for verification
                                if matches <= 10 or abs(pct_diff) > 0.15:
                                    match_indicator = (
                                        "âœ…" if best_score == 1.0 else f"ðŸ”„ ({best_score:.2f})"
                                    )
                                    print(
                                        f"   {match_indicator} {dff_name} â†’ {best_match.name}: {base_proj:.1f} â†’ {dff_proj:.1f} ({pct_diff * 100:+.0f}%)"
                                    )

                                if abs(pct_diff) > 0.15:
                                    significant_adjustments += 1
                    else:
                        no_matches.append(dff_name)

                except Exception:
                    continue

            # Show unmatched players
            if no_matches and len(no_matches) <= 10:
                print(f"\n   âŒ No matches found for: {', '.join(no_matches[:5])}")
                if len(no_matches) > 5:
                    print(f"       ... and {len(no_matches) - 5} more")

            print(f"\nâœ… DFF integration complete:")
            print(f"   Total matches: {matches}/{len(df)}")
            print(f"   Significant differences (>15%): {significant_adjustments}")

            return True

        except Exception as e:
            print(f"âŒ Error loading DFF data: {e}")
            import traceback

            traceback.print_exc()
            return False

    def _name_similarity(self, name1: str, name2: str) -> float:
        """Calculate similarity between two names"""
        return SequenceMatcher(None, name1.lower(), name2.lower()).ratio()


# Main function to apply enhanced statistical analysis
def apply_enhanced_statistical_analysis(players: List[Any], verbose: bool = False) -> int:
    """
    Apply enhanced statistical analysis to players

    Args:
        players: List of player objects
        verbose: Whether to print detailed output

    Returns:
        Number of players adjusted
    """
    if not players:
        return 0

    processor = VariableConfidenceProcessor()
    adjusted_count = 0

    if verbose:
        print(f"\nðŸ”¬ ENHANCED STATISTICAL ANALYSIS")
        print(f"=" * 60)
        print(f"Processing {len(players)} players with advanced metrics...")

    for player in players:
        try:
            # Skip if player doesn't have necessary attributes
            if not hasattr(player, "enhanced_score"):
                continue

            original_score = player.enhanced_score
            adjustment, breakdown = processor.calculate_enhanced_adjustment(player)

            if abs(adjustment) > 0.01:
                # Apply adjustment
                player.enhanced_score = original_score * (1 + adjustment)
                adjusted_count += 1

                if verbose and adjusted_count <= 10:  # Show first 10
                    print(f"\nðŸ“Š {player.name} ({player.primary_position})")
                    print(
                        f"   Original: {original_score:.2f} â†’ Adjusted: {player.enhanced_score:.2f}"
                    )

                    # Show breakdown
                    for source, details in breakdown["breakdown"].items():
                        if isinstance(details, dict) and "adjustment" in details:
                            adj_pct = details["adjustment"] * 100
                            print(f"   {source.upper()}: {adj_pct:+.1f}% adjustment")

        except Exception as e:
            if verbose:
                print(f"âš ï¸ Error processing {getattr(player, 'name', 'Unknown')}: {e}")
            continue

    if verbose:
        print(
            f"\nâœ… Statistical analysis complete: {adjusted_count}/{len(players)} players adjusted"
        )

    return adjusted_count


# Export the main components
__all__ = [
    "ConfidenceConfig",
    "EnhancedStatcastScoring",
    "VariableConfidenceProcessor",
    "DFFRankingsProcessor",
    "apply_enhanced_statistical_analysis",
]
